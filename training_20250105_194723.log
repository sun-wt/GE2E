Using device: cuda
Conformer(
  (encoder): ConformerEncoder(
    (conv_subsample): Conv2dSubampling(
      (sequential): Sequential(
        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
    )
    (input_projection): Sequential(
      (0): Linear(
        (linear): Linear(in_features=2432, out_features=128, bias=True)
      )
      (1): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): ConformerBlock(
        (sequential): Sequential(
          (0): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): ResidualConnectionModule(
            (module): MultiHeadedSelfAttentionModule(
              (positional_encoding): RelPositionalEncoding()
              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attention): RelativeMultiHeadAttention(
                (query_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (key_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (value_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (pos_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=False)
                )
                (dropout): Dropout(p=0.1, inplace=False)
                (out_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): ResidualConnectionModule(
            (module): ConformerConvModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Transpose()
                (2): PointwiseConv1d(
                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
                )
                (3): GLU()
                (4): DepthwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
                )
                (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (6): Swish()
                (7): PointwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
                )
                (8): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): ConformerBlock(
        (sequential): Sequential(
          (0): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): ResidualConnectionModule(
            (module): MultiHeadedSelfAttentionModule(
              (positional_encoding): RelPositionalEncoding()
              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attention): RelativeMultiHeadAttention(
                (query_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (key_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (value_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (pos_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=False)
                )
                (dropout): Dropout(p=0.1, inplace=False)
                (out_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): ResidualConnectionModule(
            (module): ConformerConvModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Transpose()
                (2): PointwiseConv1d(
                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
                )
                (3): GLU()
                (4): DepthwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
                )
                (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (6): Swish()
                (7): PointwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
                )
                (8): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (fc): Linear(
    (linear): Linear(in_features=128, out_features=35, bias=False)
  )
)
[Info] 初始模型權重已從 ./checkpoints/h0.pt 加載。
[Info] 保存檢查點到 ./checkpoints/20250105_194728

Epoch 1/5
Step 0, Loss: 1.9455
Step 10, Loss: 1.9166
Step 20, Loss: 1.9459
Step 30, Loss: 1.9459
Step 40, Loss: 1.9459
Step 50, Loss: 1.9459
Step 60, Loss: 1.9459
Step 70, Loss: 1.9459
Step 80, Loss: 1.9459
Step 90, Loss: 1.9459
Step 100, Loss: 1.9459
Step 110, Loss: 1.9459
Step 120, Loss: 1.9459
Step 130, Loss: 1.9459
Step 140, Loss: 1.9459
Step 150, Loss: 1.9459
Step 160, Loss: 1.9459
Step 170, Loss: 1.9459
Step 180, Loss: 1.9459
Step 190, Loss: 1.9459
Step 200, Loss: 1.9459
Step 210, Loss: 1.9459
Step 220, Loss: 1.9459
Step 230, Loss: 1.9459
Step 240, Loss: 1.9459