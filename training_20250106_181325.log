Using device: cuda
Conformer(
  (encoder): ConformerEncoder(
    (conv_subsample): Conv2dSubampling(
      (sequential): Sequential(
        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
    )
    (input_projection): Sequential(
      (0): Linear(
        (linear): Linear(in_features=2432, out_features=128, bias=True)
      )
      (1): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0): ConformerBlock(
        (sequential): Sequential(
          (0): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): ResidualConnectionModule(
            (module): MultiHeadedSelfAttentionModule(
              (positional_encoding): RelPositionalEncoding()
              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attention): RelativeMultiHeadAttention(
                (query_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (key_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (value_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (pos_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=False)
                )
                (dropout): Dropout(p=0.1, inplace=False)
                (out_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): ResidualConnectionModule(
            (module): ConformerConvModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Transpose()
                (2): PointwiseConv1d(
                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
                )
                (3): GLU()
                (4): DepthwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
                )
                (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (6): Swish()
                (7): PointwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
                )
                (8): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): ConformerBlock(
        (sequential): Sequential(
          (0): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): ResidualConnectionModule(
            (module): MultiHeadedSelfAttentionModule(
              (positional_encoding): RelPositionalEncoding()
              (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (attention): RelativeMultiHeadAttention(
                (query_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (key_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (value_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
                (pos_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=False)
                )
                (dropout): Dropout(p=0.1, inplace=False)
                (out_proj): Linear(
                  (linear): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): ResidualConnectionModule(
            (module): ConformerConvModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Transpose()
                (2): PointwiseConv1d(
                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
                )
                (3): GLU()
                (4): DepthwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
                )
                (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (6): Swish()
                (7): PointwiseConv1d(
                  (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
                )
                (8): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): ResidualConnectionModule(
            (module): FeedForwardModule(
              (sequential): Sequential(
                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (1): Linear(
                  (linear): Linear(in_features=128, out_features=512, bias=True)
                )
                (2): Swish()
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(
                  (linear): Linear(in_features=512, out_features=128, bias=True)
                )
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (fc): Linear(
    (linear): Linear(in_features=128, out_features=35, bias=False)
  )
)
[Info] 初始模型權重已從 ./checkpoints/h0.pt 加載。
[Info] 保存檢查點到 ./checkpoints/20250106_181416

Epoch 1/10
Step 0, Loss: 1.9447
Step 10, Loss: 1.8909
Step 20, Loss: 1.9337
Step 30, Loss: 1.8436
Step 40, Loss: 1.7922
Step 50, Loss: 1.7875
Step 60, Loss: 1.8932
Step 70, Loss: 1.7968
Step 80, Loss: 1.9867
Step 90, Loss: 1.8627
Step 100, Loss: 1.8492
Step 110, Loss: 1.8852
Step 120, Loss: 1.7081
Step 130, Loss: 1.8205
Step 140, Loss: 1.8440
Step 150, Loss: 1.8507
Step 160, Loss: 1.8494
Step 170, Loss: 1.8323
Step 180, Loss: 1.8038
Step 190, Loss: 1.7300
Step 200, Loss: 1.6980
Step 210, Loss: 1.8759
Step 220, Loss: 1.7645
Step 230, Loss: 1.8201
Step 240, Loss: 1.7246
Step 250, Loss: 1.7596
Step 260, Loss: 1.8357
Step 270, Loss: 1.7951
Step 280, Loss: 1.9060
Step 290, Loss: 1.7030
Step 300, Loss: 1.9052
Step 310, Loss: 1.7175
Step 320, Loss: 1.8082
Step 330, Loss: 1.7974
Step 340, Loss: 1.7612
Step 350, Loss: 1.7361
Step 360, Loss: 1.7044
Step 370, Loss: 1.7612
Step 380, Loss: 1.6788
Step 390, Loss: 1.8219
Step 400, Loss: 1.7464
Step 410, Loss: 1.7964
Step 420, Loss: 1.8716
Step 430, Loss: 1.7276
Step 440, Loss: 1.6614
Step 450, Loss: 1.7472
Step 460, Loss: 1.6916
Step 470, Loss: 1.7469
Step 480, Loss: 1.8442
Step 490, Loss: 1.6995
Step 500, Loss: 1.8471
Step 510, Loss: 1.7756
Step 520, Loss: 1.6933
Step 530, Loss: 1.6777
Step 540, Loss: 1.8364
Step 550, Loss: 1.8064
Step 560, Loss: 1.6757
Step 570, Loss: 1.7933
Step 580, Loss: 1.7379
Step 590, Loss: 1.7432
Step 600, Loss: 1.7020
Step 610, Loss: 1.6617
Step 620, Loss: 1.7269
Step 630, Loss: 1.8031
Step 640, Loss: 1.6384
Step 650, Loss: 1.7684
Step 660, Loss: 1.7093
Step 670, Loss: 1.7091
Step 680, Loss: 1.7685
Step 690, Loss: 1.7729
Step 700, Loss: 1.6817
Step 710, Loss: 1.6569
Step 720, Loss: 1.7615
Step 730, Loss: 1.7644
Step 740, Loss: 1.8202
Step 750, Loss: 1.7141
Step 760, Loss: 1.5772
Step 770, Loss: 1.7339
Step 780, Loss: 1.7377
Step 790, Loss: 1.6891
Step 800, Loss: 1.8104
Step 810, Loss: 1.7675
Step 820, Loss: 1.7580
Step 830, Loss: 1.6266
Step 840, Loss: 1.7190
Step 850, Loss: 1.7713
Step 860, Loss: 1.8145
Step 870, Loss: 1.7979
Step 880, Loss: 1.7690
Step 890, Loss: 1.8280
Step 900, Loss: 1.8170
Step 910, Loss: 1.7718
Step 920, Loss: 1.6514
Step 930, Loss: 1.7183
Step 940, Loss: 1.6694
Step 950, Loss: 1.7450
Step 960, Loss: 1.6699
Step 970, Loss: 1.7148
Step 980, Loss: 1.7382
Step 990, Loss: 1.7007
Step 1000, Loss: 1.6791
Step 1010, Loss: 1.6348
Step 1020, Loss: 1.7710
Step 1030, Loss: 1.6829
Step 1040, Loss: 1.8474
Step 1050, Loss: 1.8100
Step 1060, Loss: 1.6551
Step 1070, Loss: 1.6418
Step 1080, Loss: 1.7576
Step 1090, Loss: 1.7465
Step 1100, Loss: 1.7616
Step 1110, Loss: 1.7789
Step 1120, Loss: 1.6949
Step 1130, Loss: 1.6453
Step 1140, Loss: 1.8063
Step 1150, Loss: 1.6557
Step 1160, Loss: 1.8194
Step 1170, Loss: 1.7405
Step 1180, Loss: 1.6644
Step 1190, Loss: 1.6727
Step 1200, Loss: 1.7037
Step 1210, Loss: 1.5777
Step 1220, Loss: 1.6882
Step 1230, Loss: 1.8051
Step 1240, Loss: 1.6791
Step 1250, Loss: 1.7384
Step 1260, Loss: 1.7454
Step 1270, Loss: 1.6692
Step 1280, Loss: 1.5937
Step 1290, Loss: 1.8215
Step 1300, Loss: 1.7589
Step 1310, Loss: 1.7647
Step 1320, Loss: 1.7306
Step 1330, Loss: 1.6616
Step 1340, Loss: 1.6868
Step 1350, Loss: 1.6941
Step 1360, Loss: 1.7012
Step 1370, Loss: 1.6731
Step 1380, Loss: 1.6583
Step 1390, Loss: 1.6130
Step 1400, Loss: 1.5969
Step 1410, Loss: 1.6921
Step 1420, Loss: 1.5722
Step 1430, Loss: 1.7065
Step 1440, Loss: 1.6079
Step 1450, Loss: 1.7292
Step 1460, Loss: 1.6849
Step 1470, Loss: 1.6619
Step 1480, Loss: 1.6201
Step 1490, Loss: 1.5673
Step 1500, Loss: 1.6910
Step 1510, Loss: 1.7479
Step 1520, Loss: 1.6482
Step 1530, Loss: 1.6990
Step 1540, Loss: 1.6541
Step 1550, Loss: 1.7159
Step 1560, Loss: 1.7944
Step 1570, Loss: 1.6820
Step 1580, Loss: 1.7483
Step 1590, Loss: 1.6363
Step 1600, Loss: 1.8469
Step 1610, Loss: 1.7879
Step 1620, Loss: 1.7236
Step 1630, Loss: 1.6005
Step 1640, Loss: 1.8258
Step 1650, Loss: 1.6208
Step 1660, Loss: 1.6350
Step 1670, Loss: 1.6637
Step 1680, Loss: 1.6153
Step 1690, Loss: 1.6708
Step 1700, Loss: 1.6422
Step 1710, Loss: 1.7707
Step 1720, Loss: 1.7318
Step 1730, Loss: 1.6988
Step 1740, Loss: 1.7501
Step 1750, Loss: 1.6415
Step 1760, Loss: 1.7140
Step 1770, Loss: 1.6225
Step 1780, Loss: 1.6118
Step 1790, Loss: 1.6626
Step 1800, Loss: 1.6363
Step 1810, Loss: 1.7383
Step 1820, Loss: 1.7620
Step 1830, Loss: 1.7380
Step 1840, Loss: 1.7653
Step 1850, Loss: 1.6903
Step 1860, Loss: 1.7588
Step 1870, Loss: 1.7654
Step 1880, Loss: 1.6533
Step 1890, Loss: 1.6601
Step 1900, Loss: 1.5742
Step 1910, Loss: 1.6713
Step 1920, Loss: 1.6471
Step 1930, Loss: 1.7341
Step 1940, Loss: 1.5593
Step 1950, Loss: 1.7450
Step 1960, Loss: 1.5788
Step 1970, Loss: 1.7383
Step 1980, Loss: 1.6631
Step 1990, Loss: 1.7313
Epoch 1, Average Loss: 1.7287
[Info] 模型檢查點已保存至 ./checkpoints/20250106_181416/epoch_1.pt

Epoch 2/10
Step 0, Loss: 1.6635
Step 10, Loss: 1.6195
Step 20, Loss: 1.6974
Step 30, Loss: 1.7612
Step 40, Loss: 1.6866
Step 50, Loss: 1.6183
Step 60, Loss: 1.7112
Step 70, Loss: 1.7666
Step 80, Loss: 1.8292
Step 90, Loss: 1.6786
Step 100, Loss: 1.7344
Step 110, Loss: 1.6908
Step 120, Loss: 1.6241
Step 130, Loss: 1.5766
Step 140, Loss: 1.7964
Step 150, Loss: 1.8087
Step 160, Loss: 1.6031
Step 170, Loss: 1.6047
Step 180, Loss: 1.6179
Step 190, Loss: 1.6193
Step 200, Loss: 1.5803
Step 210, Loss: 1.7044
Step 220, Loss: 1.7063
Step 230, Loss: 1.7194
Step 240, Loss: 1.6657
Step 250, Loss: 1.6955
Step 260, Loss: 1.6850
Step 270, Loss: 1.7781
Step 280, Loss: 1.7135
Step 290, Loss: 1.6611
Step 300, Loss: 1.6671
Step 310, Loss: 1.6062
Step 320, Loss: 1.6932
Step 330, Loss: 1.6773
Step 340, Loss: 1.7163
Step 350, Loss: 1.6722
Step 360, Loss: 1.6698
Step 370, Loss: 1.6835
Step 380, Loss: 1.7105
Step 390, Loss: 1.6721
Step 400, Loss: 1.6635
Step 410, Loss: 1.7080
Step 420, Loss: 1.6339
Step 430, Loss: 1.5727
Step 440, Loss: 1.6433
Step 450, Loss: 1.6601
Step 460, Loss: 1.6662
Step 470, Loss: 1.7263
Step 480, Loss: 1.6514
Step 490, Loss: 1.6884
Step 500, Loss: 1.7999
Step 510, Loss: 1.5647
Step 520, Loss: 1.6756
Step 530, Loss: 1.6366
Step 540, Loss: 1.8101
Step 550, Loss: 1.8427
Step 560, Loss: 1.6885
Step 570, Loss: 1.7133
Step 580, Loss: 1.7553
Step 590, Loss: 1.6983
Step 600, Loss: 1.5878
Step 610, Loss: 1.5946
Step 620, Loss: 1.7267
Step 630, Loss: 1.7797
Step 640, Loss: 1.6214
Step 650, Loss: 1.6893
Step 660, Loss: 1.6930
Step 670, Loss: 1.6609
Step 680, Loss: 1.6807
Step 690, Loss: 1.7167
Step 700, Loss: 1.6319
Step 710, Loss: 1.5814
Step 720, Loss: 1.6801
Step 730, Loss: 1.7070
Step 740, Loss: 1.7055
Step 750, Loss: 1.6661
Step 760, Loss: 1.5502
Step 770, Loss: 1.7554
Step 780, Loss: 1.7493
Step 790, Loss: 1.6311
Step 800, Loss: 1.6317
Step 810, Loss: 1.6919
Step 820, Loss: 1.6675
Step 830, Loss: 1.6531
Step 840, Loss: 1.7167
Step 850, Loss: 1.6938
Step 860, Loss: 1.6832
Step 870, Loss: 1.7346
Step 880, Loss: 1.6613
Step 890, Loss: 1.7963
Step 900, Loss: 1.7392
Step 910, Loss: 1.7690
Step 920, Loss: 1.6381
Step 930, Loss: 1.6689
Step 940, Loss: 1.6018
Step 950, Loss: 1.5767
Step 960, Loss: 1.5715
Step 970, Loss: 1.6572
Step 980, Loss: 1.7596
Step 990, Loss: 1.6652
Step 1000, Loss: 1.6727
Step 1010, Loss: 1.5696
Step 1020, Loss: 1.6956
Step 1030, Loss: 1.7039
Step 1040, Loss: 1.7543
Step 1050, Loss: 1.6206
Step 1060, Loss: 1.7685
Step 1070, Loss: 1.6863
Step 1080, Loss: 1.6652
Step 1090, Loss: 1.7179
Step 1100, Loss: 1.5736
Step 1110, Loss: 1.6785
Step 1120, Loss: 1.6115
Step 1130, Loss: 1.6393
Step 1140, Loss: 1.7675
Step 1150, Loss: 1.6401
Step 1160, Loss: 1.7759
Step 1170, Loss: 1.7300
Step 1180, Loss: 1.5859
Step 1190, Loss: 1.6697
Step 1200, Loss: 1.6212
Step 1210, Loss: 1.5961
Step 1220, Loss: 1.6229
Step 1230, Loss: 1.7104
Step 1240, Loss: 1.6643
Step 1250, Loss: 1.6229
Step 1260, Loss: 1.6626
Step 1270, Loss: 1.7474
Step 1280, Loss: 1.6360
Step 1290, Loss: 1.7145
Step 1300, Loss: 1.6965
Step 1310, Loss: 1.6609
Step 1320, Loss: 1.7073
Step 1330, Loss: 1.6697
Step 1340, Loss: 1.7018
Step 1350, Loss: 1.6632
Step 1360, Loss: 1.6075
Step 1370, Loss: 1.5784
Step 1380, Loss: 1.6035
Step 1390, Loss: 1.6114
Step 1400, Loss: 1.5432
Step 1410, Loss: 1.7491
Step 1420, Loss: 1.5594
Step 1430, Loss: 1.6399
Step 1440, Loss: 1.5920
Step 1450, Loss: 1.7298
Step 1460, Loss: 1.5818
Step 1470, Loss: 1.7026
Step 1480, Loss: 1.5619
Step 1490, Loss: 1.6425
Step 1500, Loss: 1.7453
Step 1510, Loss: 1.6621
Step 1520, Loss: 1.7569
Step 1530, Loss: 1.7270
Step 1540, Loss: 1.6289
Step 1550, Loss: 1.7315
Step 1560, Loss: 1.7740
Step 1570, Loss: 1.6185
Step 1580, Loss: 1.7705
Step 1590, Loss: 1.6299
Step 1600, Loss: 1.7737
Step 1610, Loss: 1.7049
Step 1620, Loss: 1.7890
Step 1630, Loss: 1.5836
Step 1640, Loss: 1.7267
Step 1650, Loss: 1.6786
Step 1660, Loss: 1.5541
Step 1670, Loss: 1.6784
Step 1680, Loss: 1.6279
Step 1690, Loss: 1.6540