PyTorch 版本: 1.13.1+cu117
Using device: cuda
Grouping data by WORD...
ConformerEncoder(
  (conv_subsample): Conv2dSubampling(
    (sequential): Sequential(
      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
      (1): ReLU()
      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
      (3): ReLU()
    )
  )
  (input_projection): Sequential(
    (0): Linear(
      (linear): Linear(in_features=2432, out_features=128, bias=True)
    )
    (1): Dropout(p=0.1, inplace=False)
  )
  (layers): ModuleList(
    (0): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): RelPositionalEncoding()
            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
              )
              (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): RelPositionalEncoding()
            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
              )
              (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
[Info] 初始模型權重已保存至 ./checkpoints/20250109_095602/epoch_20.pt

Epoch 1/20
Selected words: ['worcestershire', 'contend', 'scandinavian', 'conventional', 'davies', 'buns', 'christ', 'miami']
Step 0, Loss: 1.9294
Selected words: ['fillet', 'conference', 'increased', 'kiln', 'cricketer', 'fidget', 'giles', 'lodging']
Selected words: ['longstanding', 'grease', 'articulated', 'foremost', 'soto', 'debuted', 'conjecture', 'libya']
Selected words: ['informative', 'reply', 'gleamed', 'casper', 'escape', 'anita', 'corporations', 'mab']
Selected words: ['sneaking', 'under', 'helicopters', 'snowden', 'governed', 'interviews', 'possess', 'renewables']
Selected words: ['deaths', 'neutralize', 'extinct', 'era', 'configuration', 'linguists', 'pope', 'forces']
Selected words: ['scaling', 'debut', 'dismissed', 'burbank', 'deafening', 'multiedit', 'unconvincing', 'wooden']
Selected words: ['duration', 'postwar', 'grill', 'monorail', 'equal', 'giraffes', 'finalist', 'prospect']
Selected words: ['coma', 'tropics', 'landscaped', 'discusses', 'yiddish', 'misspelled', 'massacred', 'cringe']
Selected words: ['indication', 'piece', 'kite', 'filmed', 'beverley', 'programmes', 'transistors', 'disasters']