PyTorch 版本: 1.13.1+cu117
Using device: cuda
Grouping data by WORD...
[Info] 使用 Tiny Conformer 模型
ConformerEncoder(
  (conv_subsample): Conv2dSubampling(
    (sequential): Sequential(
      (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))
      (1): ReLU()
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
      (3): ReLU()
    )
  )
  (input_projection): Sequential(
    (0): Linear(
      (linear): Linear(in_features=2304, out_features=256, bias=True)
    )
    (1): Dropout(p=0.1, inplace=False)
  )
  (layers): ModuleList(
    (0): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (4): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (5): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): PositionalEncoding()
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,), groups=256, bias=False)
              )
              (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=256, out_features=1024, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=1024, out_features=256, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
