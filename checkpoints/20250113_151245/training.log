PyTorch 版本: 1.13.1+cu117
Using device: cuda
Grouping data by WORD...
ConformerEncoder(
  (conv_subsample): Conv2dSubampling(
    (sequential): Sequential(
      (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(2, 2))
      (1): ReLU()
      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
      (3): ReLU()
    )
  )
  (input_projection): Sequential(
    (0): Linear(
      (linear): Linear(in_features=2432, out_features=128, bias=True)
    )
    (1): Dropout(p=0.1, inplace=False)
  )
  (layers): ModuleList(
    (0): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): RelPositionalEncoding()
            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
              )
              (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): ConformerBlock(
      (sequential): Sequential(
        (0): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (1): ResidualConnectionModule(
          (module): MultiHeadedSelfAttentionModule(
            (positional_encoding): RelPositionalEncoding()
            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attention): RelativeMultiHeadAttention(
              (query_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (key_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (value_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
              (pos_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=False)
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (out_proj): Linear(
                (linear): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ResidualConnectionModule(
          (module): ConformerConvModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Transpose()
              (2): PointwiseConv1d(
                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
              )
              (3): GLU()
              (4): DepthwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(31,), stride=(1,), padding=(15,), groups=128, bias=False)
              )
              (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (6): Swish()
              (7): PointwiseConv1d(
                (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
              )
              (8): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (3): ResidualConnectionModule(
          (module): FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
              (1): Linear(
                (linear): Linear(in_features=128, out_features=512, bias=True)
              )
              (2): Swish()
              (3): Dropout(p=0.1, inplace=False)
              (4): Linear(
                (linear): Linear(in_features=512, out_features=128, bias=True)
              )
              (5): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
[Info] 初始模型權重已從 ./checkpoints/e0.pt 加載。

Epoch 1/10
Selected words: ['compartment', 'murals', 'capitalist', 'newcastle', 'blonde', 'retrospective', 'recap', 'attended']
